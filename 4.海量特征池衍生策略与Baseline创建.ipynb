{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7305b8da-66de-472d-9a77-66eecbb3e6fc",
   "metadata": {},
   "source": [
    "## <center> Filter特征筛选+随机森林建模+网格搜索调优"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72593a2-b30d-4565-b227-69e1f2896179",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来首先尝试Filter特征筛选+随机森林建模+网格搜索调优的策略来训练第一组模型，当然，在Part I的结尾我们曾尝试随机森林建模，现在再次调用随机森林来建模，一方面是快速回顾此前的内容，同时由于接下来需要进行模型融合，因此我们需要手动保存一些模型训练的中间结果，因此需要改写Part I中的建模流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b31d3-1a56-4670-b5fe-89de02ff817c",
   "metadata": {},
   "source": [
    "### 1.Filter特征筛选"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09430a6-98af-4886-be61-1ac464c2c729",
   "metadata": {},
   "source": [
    "- 特征筛选思路与方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6898a2-c588-49ab-89f6-571d2ecd2d83",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在特征总数较多且特征矩阵较为稀疏时，需要考虑在模型训练前进行特征筛选。我们知道，树模型及树模型的集成模型存在一定的特征筛选机制，即每棵树在进行训练的时候会优先选择能最大程度提升子集纯度的特征进行划分，但当特征太多时，尽管最终结果不一定会受到冗余（无用）特征影响，但模型效率会大幅降低，因此面对树模型及树模型的集成模型，我们仍然需要考虑在实际建模前进行特征筛选，优先带入有效特征进行建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1571653a-f13e-492c-920c-e6ab7418d9c5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而一般来说，特征筛选的方式主要有两类，其一是通过某些统计量对特征进行评估，在实际模型训练开始之前挑选出那些更加有效的特征并最终完成筛选，例如我们可以通过相关系数计算，判断特征和标签之间的相关关系，然后选取相关系数较大的特征带入进行建模，这种方法也被称为Filter方法；此外，我们也可以通过模型来筛选有效特征，例如随机森林模型可以输出特征重要性，我们可以先快速训练一个随机森林模型，然后根据输出的特征重要性，筛选更重要的特征带入后续超参数优化及交叉验证过程（需要知道的是，对于单独一个模型来说，是否带入冗余特征对单次训练来说影响不大，但由于超参数优化和交叉验证需要重复进行多轮训练，此时冗余特征的影响就会指数级上升），这样的特征筛选过程也被称为Wrapper过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a9986-68a4-4327-9eaa-a7be2450a531",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在接下来的随机森林与LightGBM的模型训练过程中，我们将分别使用Filter方法和Wrapper方法进行特征筛选，但其实不同方法也是可以互换的，即我们也可以采用RF+Filter和LightGBM+Wrapper策略，同学们可以课后自行尝试这些组合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7df13-a446-45d2-a620-e8e94fa577b4",
   "metadata": {},
   "source": [
    "- Filter相关系数特征筛选过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e024b6d-99bb-4a81-ba15-23a1196cb2dc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们尝试使用相关系数进行特征筛选。在目前处理好的数据中，特征总数高达1742，并且特征矩阵整体较为稀疏："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "609885e5-e8e4-4466-b838-322e9b925cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('preprocess/train.csv')\n",
    "test = pd.read_csv('preprocess/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4cff81-0bfd-46e4-8ffa-64202d892ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201917, 1742)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "188b0516-d72c-42f5-97c2-85645dfcd2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "      <th>authorized_flag&amp;1&amp;purchase_amount</th>\n",
       "      <th>authorized_flag&amp;1&amp;installments</th>\n",
       "      <th>city_id&amp;19&amp;purchase_amount</th>\n",
       "      <th>city_id&amp;19&amp;installments</th>\n",
       "      <th>...</th>\n",
       "      <th>category_4_var</th>\n",
       "      <th>category_4_skew</th>\n",
       "      <th>category_4_sum</th>\n",
       "      <th>city_id_nunique</th>\n",
       "      <th>merchant_category_id_nunique</th>\n",
       "      <th>merchant_id_nunique</th>\n",
       "      <th>state_id_nunique</th>\n",
       "      <th>subsector_id_nunique</th>\n",
       "      <th>card_id_size</th>\n",
       "      <th>card_id_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820283</td>\n",
       "      <td>-170.641218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.422815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054623</td>\n",
       "      <td>-3.811953</td>\n",
       "      <td>261.0</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "      <td>118</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>283</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>-213.239185</td>\n",
       "      <td>507.0</td>\n",
       "      <td>-4.782308</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075036</td>\n",
       "      <td>-3.073118</td>\n",
       "      <td>327.0</td>\n",
       "      <td>9</td>\n",
       "      <td>58</td>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>356</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688056</td>\n",
       "      <td>-28.528749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.705405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065011</td>\n",
       "      <td>-3.548480</td>\n",
       "      <td>41.0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142495</td>\n",
       "      <td>-54.145736</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-0.707839</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023523</td>\n",
       "      <td>-6.361110</td>\n",
       "      <td>82.0</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.159749</td>\n",
       "      <td>-88.966702</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091496</td>\n",
       "      <td>-2.668681</td>\n",
       "      <td>151.0</td>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>103</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>169</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1742 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0                  67  C_ID_92a2005557          5          2          1   \n",
       "1                  62  C_ID_3d0044924f          4          1          0   \n",
       "2                  57  C_ID_d639edf6cd          2          2          0   \n",
       "3                  70  C_ID_186d6a6901          4          3          0   \n",
       "4                  72  C_ID_cdbd2c0db2          1          3          0   \n",
       "\n",
       "     target  authorized_flag&1&purchase_amount  \\\n",
       "0 -0.820283                        -170.641218   \n",
       "1  0.392913                        -213.239185   \n",
       "2  0.688056                         -28.528749   \n",
       "3  0.142495                         -54.145736   \n",
       "4 -0.159749                         -88.966702   \n",
       "\n",
       "   authorized_flag&1&installments  city_id&19&purchase_amount  \\\n",
       "0                             0.0                   -1.422815   \n",
       "1                           507.0                   -4.782308   \n",
       "2                             0.0                   -0.705405   \n",
       "3                            89.0                   -0.707839   \n",
       "4                           179.0                    0.000000   \n",
       "\n",
       "   city_id&19&installments  ...  category_4_var  category_4_skew  \\\n",
       "0                      0.0  ...        0.054623        -3.811953   \n",
       "1                      7.0  ...        0.075036        -3.073118   \n",
       "2                      0.0  ...        0.065011        -3.548480   \n",
       "3                      1.0  ...        0.023523        -6.361110   \n",
       "4                      0.0  ...        0.091496        -2.668681   \n",
       "\n",
       "   category_4_sum  city_id_nunique  merchant_category_id_nunique  \\\n",
       "0           261.0                9                            46   \n",
       "1           327.0                9                            58   \n",
       "2            41.0                5                             9   \n",
       "3            82.0                7                            28   \n",
       "4           151.0                7                            37   \n",
       "\n",
       "   merchant_id_nunique  state_id_nunique  subsector_id_nunique  card_id_size  \\\n",
       "0                  118                 3                    21           283   \n",
       "1                  148                 3                    24           356   \n",
       "2                   14                 2                     8            44   \n",
       "3                   57                 5                    15            84   \n",
       "4                  103                 7                    19           169   \n",
       "\n",
       "   card_id_count  \n",
       "0            283  \n",
       "1            356  \n",
       "2             44  \n",
       "3             84  \n",
       "4            169  \n",
       "\n",
       "[5 rows x 1742 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f58ee760-1076-4c9d-adb2-8c7a7ff08e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7739293811412331"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算稀疏性\n",
    "1 - np.count_nonzero(train) / train.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b2847-e38a-4151-adf1-5bde08ddc247",
   "metadata": {},
   "source": [
    "因此，我们需要在实际建模之前进行特征筛选，以及排除过于稀疏的特征。考虑到标签是连续变量，此处可以直接使用皮尔逊相关系数来进行特征筛选，筛选过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86242e6d-2519-4d17-9ace-a666c94033a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 提取特征名称\n",
    "features = train.columns.tolist()\n",
    "features.remove(\"card_id\")\n",
    "features.remove(\"target\")\n",
    "featureSelect = features[:]\n",
    "\n",
    "# 计算相关系数\n",
    "corr = []\n",
    "for fea in featureSelect:\n",
    "    corr.append(abs(train[[fea, 'target']].fillna(0).corr().values[0][1]))\n",
    "\n",
    "# 取top300的特征进行建模，具体数量可选\n",
    "se = pd.Series(corr, index=featureSelect).sort_values(ascending=False)\n",
    "feature_select = ['card_id'] + se[:300].index.tolist()\n",
    "\n",
    "# 输出结果\n",
    "train_RF = train[feature_select + ['target']]\n",
    "test_RF = test[feature_select]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a8bd8a-9677-426c-ad2e-6e0fdb12eb9d",
   "metadata": {},
   "source": [
    "最终生成的train_RF、test_RF就将是后续带入随机森林建模的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6658619-f725-4617-a187-64b1042e63e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>purchase_month_max_hist</th>\n",
       "      <th>purchase_month_mean_hist</th>\n",
       "      <th>purchase_month_max</th>\n",
       "      <th>purchase_month_mean</th>\n",
       "      <th>purchase_month_min_new</th>\n",
       "      <th>purchase_hour_section_nunique_new</th>\n",
       "      <th>purchase_month_mean_new</th>\n",
       "      <th>purchase_month_min</th>\n",
       "      <th>purchase_month_min_hist</th>\n",
       "      <th>...</th>\n",
       "      <th>city_id&amp;151&amp;installments</th>\n",
       "      <th>merchant_category_id&amp;836&amp;purchase_amount</th>\n",
       "      <th>merchant_category_id&amp;80&amp;installments</th>\n",
       "      <th>purchase_day_max_hist</th>\n",
       "      <th>most_recent_purchases_range_skew</th>\n",
       "      <th>merchant_category_id&amp;683&amp;installments</th>\n",
       "      <th>purchase_amount_nunique</th>\n",
       "      <th>category_3&amp;0&amp;installments</th>\n",
       "      <th>purchase_day_diff_nunique</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>12</td>\n",
       "      <td>8.708861</td>\n",
       "      <td>15</td>\n",
       "      <td>9.526502</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.739130</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.390758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.820283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>11</td>\n",
       "      <td>6.110368</td>\n",
       "      <td>14</td>\n",
       "      <td>7.078652</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.157895</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.898998</td>\n",
       "      <td>25.0</td>\n",
       "      <td>238</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.392913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>12</td>\n",
       "      <td>4.190476</td>\n",
       "      <td>15</td>\n",
       "      <td>4.636364</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.676919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.688056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>12</td>\n",
       "      <td>9.426230</td>\n",
       "      <td>15</td>\n",
       "      <td>10.547619</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.521739</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062816</td>\n",
       "      <td>6.0</td>\n",
       "      <td>78</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.142495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>12</td>\n",
       "      <td>11.255102</td>\n",
       "      <td>15</td>\n",
       "      <td>12.319527</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.788732</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.457565</td>\n",
       "      <td>3.0</td>\n",
       "      <td>156</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.159749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id  purchase_month_max_hist  purchase_month_mean_hist  \\\n",
       "0  C_ID_92a2005557                       12                  8.708861   \n",
       "1  C_ID_3d0044924f                       11                  6.110368   \n",
       "2  C_ID_d639edf6cd                       12                  4.190476   \n",
       "3  C_ID_186d6a6901                       12                  9.426230   \n",
       "4  C_ID_cdbd2c0db2                       12                 11.255102   \n",
       "\n",
       "   purchase_month_max  purchase_month_mean  purchase_month_min_new  \\\n",
       "0                  15             9.526502                    13.0   \n",
       "1                  14             7.078652                    12.0   \n",
       "2                  15             4.636364                    13.0   \n",
       "3                  15            10.547619                    13.0   \n",
       "4                  15            12.319527                    13.0   \n",
       "\n",
       "   purchase_hour_section_nunique_new  purchase_month_mean_new  \\\n",
       "0                                4.0                13.739130   \n",
       "1                                3.0                12.157895   \n",
       "2                                2.0                14.000000   \n",
       "3                                3.0                13.521739   \n",
       "4                                4.0                13.788732   \n",
       "\n",
       "   purchase_month_min  purchase_month_min_hist  ...  city_id&151&installments  \\\n",
       "0                   5                        5  ...                       0.0   \n",
       "1                   0                        0  ...                       0.0   \n",
       "2                   0                        0  ...                       0.0   \n",
       "3                   8                        8  ...                       0.0   \n",
       "4                  10                       10  ...                       0.0   \n",
       "\n",
       "   merchant_category_id&836&purchase_amount  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       0.0   \n",
       "3                                       0.0   \n",
       "4                                       0.0   \n",
       "\n",
       "   merchant_category_id&80&installments  purchase_day_max_hist  \\\n",
       "0                                   1.0                      1   \n",
       "1                                  48.0                      1   \n",
       "2                                   0.0                      1   \n",
       "3                                  12.0                      1   \n",
       "4                                  19.0                      1   \n",
       "\n",
       "   most_recent_purchases_range_skew  merchant_category_id&683&installments  \\\n",
       "0                          0.390758                                    0.0   \n",
       "1                          0.898998                                   25.0   \n",
       "2                          2.676919                                    0.0   \n",
       "3                          0.062816                                    6.0   \n",
       "4                         -0.457565                                    3.0   \n",
       "\n",
       "   purchase_amount_nunique  category_3&0&installments  \\\n",
       "0                      227                        0.0   \n",
       "1                      238                       -2.0   \n",
       "2                       42                        0.0   \n",
       "3                       78                       -3.0   \n",
       "4                      156                       -1.0   \n",
       "\n",
       "   purchase_day_diff_nunique    target  \n",
       "0                          3 -0.820283  \n",
       "1                          3  0.392913  \n",
       "2                          3  0.688056  \n",
       "3                          3  0.142495  \n",
       "4                          3 -0.159749  \n",
       "\n",
       "[5 rows x 302 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_RF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41d629cc-73a9-43a3-b105-8616968e46a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201917, 302)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_RF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f3e2d-9a82-44e8-931a-1c034cd54ddb",
   "metadata": {},
   "source": [
    "当然，我们也可以将上述过程封装为一个函数并写入Elo.py模块（自定义模块），方便后续反复调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "012c46ad-018a-4c3b-83ad-148e3f650506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_pearson(train, test):\n",
    "    \"\"\"\n",
    "    利用pearson系数进行相关性特征选择\n",
    "    :param train:训练集\n",
    "    :param test:测试集\n",
    "    :return:经过特征选择后的训练集与测试集\n",
    "    \"\"\"\n",
    "    print('feature_select...')\n",
    "    features = train.columns.tolist()\n",
    "    features.remove(\"card_id\")\n",
    "    features.remove(\"target\")\n",
    "    featureSelect = features[:]\n",
    "\n",
    "    # 去掉缺失值比例超过0.99的\n",
    "    for fea in features:\n",
    "        if train[fea].isnull().sum() / train.shape[0] >= 0.99:\n",
    "            featureSelect.remove(fea)\n",
    "\n",
    "    # 进行pearson相关性计算\n",
    "    corr = []\n",
    "    for fea in featureSelect:\n",
    "        corr.append(abs(train[[fea, 'target']].fillna(0).corr().values[0][1]))\n",
    "\n",
    "    # 取top300的特征进行建模，具体数量可选\n",
    "    se = pd.Series(corr, index=featureSelect).sort_values(ascending=False)\n",
    "    feature_select = ['card_id'] + se[:300].index.tolist()\n",
    "    print('done')\n",
    "    return train[feature_select + ['target']], test[feature_select]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2618d49b-10ee-4bad-bde3-c5ffd5d8380d",
   "metadata": {},
   "source": [
    "### 2.随机森林模型训练与超参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ececfa0-c9b1-4e17-bbf5-bf2be114995e",
   "metadata": {},
   "source": [
    "- 网格搜索方法介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258d0d41-354d-4989-9750-212cd4e15d68",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来在挑选的特征中进行模型训练，当然为了确保模型本身泛化能力，一般模型训练过程都是和超参数调优过程同步进行的，如果我们是借助sklearn框架执行上述过程，则在流程的各环节都能得到极大的简化，一种最终基本的方案就是利用网格搜索进行超参数调优。当然在sklearn中网格搜索总共有三种，分别是网格搜索（GridSearchCV）、随机网格搜索（RandomGirdSearchCV）、对半网格搜索（HalvingGridSearchCV）以及随机对半网格搜索（RandomHalvingGridSearchCV），四种网格搜索尽管流程上有差异，但基本思路一致，都是通过不断的计算各组不同超参数组合输出的最终结果，并配合交叉验证过程，来寻找一组泛化能力最强的超参数组合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4052018-3006-47c7-9cba-87a5e878ea61",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，除了网格搜索以外，常用的超参数搜索方法还有TPE搜索、贝叶斯优化器搜索等，不同于网格搜索的暴力枚举过程，这些优化器能够借助贝叶斯过程进行一定程度的先验计算，并在实际搜索过程中不断的调整先验的判断，最终通过先验的判断提前剔除（或者选择）一部分组合，进而加快整体搜索过程。本次案例中我们将率先使用网格搜索，在后续的模型中将继续展示其他优化器的超参数搜索调优过程。不过需要知道的是，在sklearn框架内，模型和优化器接口较为统一，因此整体流程会更加简洁，但从计算效率角度考虑，贝叶斯优化器的搜索效率往往更高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c8b2f5-60da-4231-bf4a-73383c9f253f",
   "metadata": {},
   "source": [
    "- 网格搜索基本流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826bf36-204b-4e0e-99bc-ab870edded82",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们可以通过如下示例快速了解网格搜索在实际挑选超参数的计算流程："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b598a12-5251-4148-8b36-8e6d3461f393",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/08/BAUqI5cf6uFavND.png\" alt=\"image-20211208154708525\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e42e5c-dd6d-47e7-95c8-e60500d42233",
   "metadata": {},
   "source": [
    "接下来我们通过定义一个完整的搜索函数来执行随机森林的网格搜索过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39a0bb78-11be-4a3f-9e5b-43351172bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45410a9c-b67c-4763-8e2f-9dde843a2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_search(train):\n",
    "    \"\"\"\n",
    "    网格搜索参数调优\n",
    "    :param train:训练集\n",
    "    :return:网格搜索训练结果\n",
    "    \"\"\"\n",
    "    # Step 1.创建网格搜索空间\n",
    "    print('param_grid_search')\n",
    "    features = train.columns.tolist()\n",
    "    features.remove(\"card_id\")\n",
    "    features.remove(\"target\")\n",
    "    parameter_space = {\n",
    "        \"n_estimators\": [81], \n",
    "        \"min_samples_leaf\": [31],\n",
    "        \"min_samples_split\": [2],\n",
    "        \"max_depth\": [10],\n",
    "        \"max_features\": [80]\n",
    "    }\n",
    "    \n",
    "    # Step 2.执行网格搜索过程\n",
    "    print(\"Tuning hyper-parameters for mse\")\n",
    "    # 实例化随机森林模型\n",
    "    clf = RandomForestRegressor(\n",
    "        criterion=\"mse\",\n",
    "        n_jobs=15,\n",
    "        random_state=22)\n",
    "    # 带入网格搜索\n",
    "    grid = GridSearchCV(clf, parameter_space, cv=2, scoring=\"neg_mean_squared_error\")\n",
    "    grid.fit(train[features].values, train['target'].values)\n",
    "    \n",
    "    # Step 3.输出网格搜索结果\n",
    "    print(\"best_params_:\")\n",
    "    print(grid.best_params_)\n",
    "    means = grid.cv_results_[\"mean_test_score\"]\n",
    "    stds = grid.cv_results_[\"std_test_score\"]\n",
    "    # 此处额外考虑观察交叉验证过程中不同超参数的\n",
    "    for mean, std, params in zip(means, stds, grid.cv_results_[\"params\"]):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc1309e8-cea6-4893-b1d7-72bb1fb7a0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_grid_search\n",
      "Tuning hyper-parameters for mse\n",
      "best_params_:\n",
      "{'max_depth': 10, 'max_features': 80, 'min_samples_leaf': 31, 'min_samples_split': 2, 'n_estimators': 81}\n",
      "-13.617 (+/-0.088) for {'max_depth': 10, 'max_features': 80, 'min_samples_leaf': 31, 'min_samples_split': 2, 'n_estimators': 81}\n"
     ]
    }
   ],
   "source": [
    "grid = param_grid_search(train_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19ee4404-54a7-4641-ae0e-3a798366c55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=RandomForestRegressor(n_jobs=15, random_state=22),\n",
       "             param_grid={'max_depth': [10], 'max_features': [80],\n",
       "                         'min_samples_leaf': [31], 'min_samples_split': [2],\n",
       "                         'n_estimators': [81]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb8391f6-1bde-4b5f-8b5e-3affe1367a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=10, max_features=80, min_samples_leaf=31,\n",
       "                      n_estimators=81, n_jobs=15, random_state=22)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e38694b-53d4-40c2-b7dd-376c1ae94894",
   "metadata": {},
   "source": [
    "至此，我们即完成了随机森林模型训练的相关工作，后续该函数也将写入Elo.py文件中。当然由于赛题要求评估指标为RMSE，因此我们可以通过如下方式计算模型最终在训练集上的RMSE计算结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1867a452-c0b4-4ee2-8e67-7697da07222b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.690154811274698"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(-grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc916c-e66a-42c2-aba7-545686ca3407",
   "metadata": {},
   "source": [
    "### 3.随机森林模型结果提交"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d9e45-b083-470c-916d-48dd91ad4177",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，最终的建模成果还是要看模型在测试集上的表现，而竞赛中测试集标签是未知的，我们只有通过在线提交测试集结果的方式才能查看最终模型在测试集上的表现，因此我们需要通过如下方式对测试集数据进行预测，并按照结果提交形式写入本地文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5e0202d-722e-4b46-bd84-c0369d3b6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['target'] = grid.best_estimator_.predict(test[features])\n",
    "test[['card_id', 'target']].to_csv(\"result/submission_randomforest.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16edf24e-6b44-45c8-a622-ed3e1317297e",
   "metadata": {},
   "source": [
    "数据文档写入完毕后，接下来就可以直接在Kaggle上提交了。上传提交结果数据和下载数据过程类似，都可以直接利用网页功能实现，或者通过命令行的方式实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3181f3c-7fd7-40e8-ac5d-36997283c563",
   "metadata": {
    "tags": []
   },
   "source": [
    "在Kaggle竞赛主页找到Late Submission进行结果提交，只需将结果文件在线提交即可："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c1bab-38ea-4c32-9e33-f6fa8df449a0",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/23/ptkMKOXhv685LxG.png\" alt=\"6a480d241a9a072ca921feedc9356c2\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f869674-ea8d-449f-b5b1-282f9e92a01d",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/23/by9scX2gFER4jT7.png\" alt=\"image-20211023183654754\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5937fc-d87e-4f6a-aafb-44a48b75e294",
   "metadata": {},
   "source": [
    "&emsp;&emsp;提交完成后，即可在我的提交结果中看到成绩了："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab569d85-b840-434e-a81c-d83150ff567f",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/23/KOqiElNHdPgGaV4.png\" alt=\"image-20211023184031717\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac189d7b-3f32-444d-a3fd-aa1593418153",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们发现，随机森林建模结果约在前40%左右，至此，我们简单跑通了一次模型，并且顺利提交并看到排名，当然一般来说，在未进行优化之前的结果，我们会称其为Baseline，即模型效果基准线，后续我们将在此基础上进一步优化模型输出结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7162c86-a42d-4b7d-890f-7831053867a4",
   "metadata": {},
   "source": [
    "### 4.随机森林交叉验证评估与中间结果保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d010df-4b17-4463-a4ee-6424199f0126",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在实际模型优化的过程中，有很多方法可以考虑，包括使用更加复杂高效的模型、进行模型融合、特征优化等等，但除此以外，还有一类经常被忽视但又同样高效优化的方法，那就是借助交叉验证进行多模型结果集成，当然此处所谓的多模型并不是采用了不同的评估器，而是同一个评估器（例如随机森林）在不同数据集上进行多次训练后生成多个模型，然后借助多个模型对测试集数据输出预测结果，最终通过取均值的方式来计算最终模型对测试集的预测结果。例如当前我们已经挑选了一组最优超参数，那么接下来就可以在这组超参数基础上进行五折交叉验证模型训练，该过程中对验证集的预测结果可以参与到后续Stacking融合过程中，而对测试集的预测结果则可以作为最终预测结果进行提交，相关过程如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ba402-e0b3-467b-a2a9-22423f40aa3d",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/08/ALF3cfuSwmB7b8z.png\" alt=\"image-20211208192640281\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9a8ca6-3c1a-4b9b-9951-6d0411b32763",
   "metadata": {},
   "source": [
    "当然，交叉验证可以直接调用sklearn中评估器来实现，我们可以通过如下代码实现上述过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "38a45ecb-95d2-4821-b39e-b18d56017ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from numpy.random import RandomState\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "97223203-283f-4307-b53a-4498cfc723ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(train, test, best_clf):\n",
    "    \"\"\"\n",
    "    进行训练和预测输出结果\n",
    "    :param train:训练集\n",
    "    :param test:测试集\n",
    "    :param best_clf:最优的分类器模型\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1.选择特征\n",
    "    print('train_predict...')\n",
    "    features = train.columns.tolist()\n",
    "    features.remove(\"card_id\")\n",
    "    features.remove(\"target\")\n",
    "\n",
    "    # Step 2.创建存储器\n",
    "    # 测试集评分存储器\n",
    "    prediction_test = 0\n",
    "    # 交叉验证评分存储器\n",
    "    cv_score = []\n",
    "    # 验证集的预测结果\n",
    "    prediction_train = pd.Series()\n",
    "    \n",
    "    # Step 3.交叉验证\n",
    "    # 实例化交叉验证评估器\n",
    "    kf = KFold(n_splits=5, random_state=22, shuffle=True)\n",
    "    # 执行交叉验证过程\n",
    "    for train_part_index, eval_index in kf.split(train[features], train['target']):\n",
    "        # 在训练集上训练模型\n",
    "        best_clf.fit(train[features].loc[train_part_index].values, train['target'].loc[train_part_index].values)\n",
    "        # 模型训练完成后，输出测试集上预测结果并累加至prediction_test中\n",
    "        prediction_test += best_clf.predict(test[features].values)\n",
    "        # 输出验证集上预测结果，eval_pre为临时变量\n",
    "        eval_pre = best_clf.predict(train[features].loc[eval_index].values)\n",
    "        # 输出验证集上预测结果评分，评估指标为MSE\n",
    "        score = np.sqrt(mean_squared_error(train['target'].loc[eval_index].values, eval_pre))\n",
    "        # 将本轮验证集上的MSE计算结果添加至cv_score列表中\n",
    "        cv_score.append(score)\n",
    "        print(score)\n",
    "        # 将验证集上的预测结果放到prediction_train中\n",
    "        prediction_train = prediction_train.append(pd.Series(best_clf.predict(train[features].loc[eval_index]),\n",
    "                                                             index=eval_index))\n",
    "    \n",
    "    # 打印每轮验证集得分、5轮验证集的平均得分\n",
    "    print(cv_score, sum(cv_score) / 5)\n",
    "    # 验证集上预测结果写入本地文件\n",
    "    pd.Series(prediction_train.sort_index().values).to_csv(\"preprocess/train_randomforest.csv\", index=False)\n",
    "    # 测试集上平均得分写入本地文件\n",
    "    pd.Series(prediction_test / 5).to_csv(\"preprocess/test_randomforest.csv\", index=False)\n",
    "    # 在测试集上加入target，也就是预测标签\n",
    "    test['target'] = prediction_test / 5\n",
    "    # 将测试集id和标签组成新的DataFrame并写入本地文件，该文件就是后续提交结果\n",
    "    test[['card_id', 'target']].to_csv(\"result/submission_randomforest.csv\", index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dcc733-ea26-4bd8-8631-7e7d88bb54a2",
   "metadata": {},
   "source": [
    "接下来尝试测试函数效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c26eb5e1-b9e0-40f1-92a2-0a8ad9fae5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_predict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-3ec77b548cfd>:16: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  prediction_train = pd.Series()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.675458048156077\n",
      "3.7098960303168167\n",
      "3.7175960057854875\n",
      "3.682888749975916\n",
      "3.646825949050688\n",
      "[3.675458048156077, 3.7098960303168167, 3.7175960057854875, 3.682888749975916, 3.646825949050688] 3.686532956656997\n"
     ]
    }
   ],
   "source": [
    "train_predict(train_RF, test_RF, grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28cddd4-0961-4650-b8a2-eccd244ec2be",
   "metadata": {},
   "source": [
    "此时，在本地文件中就能看到一个新的结果文件，该预测结果是交叉验证后各模型的预测结果的均值，相当于是一次简单的“集成”，我们可以继续在kaggle平台上提交该结果，查看测试集最终结果，能够发现，上述手动集成确实有效，前者是单模型结果，后者是手动集成后的模型结果："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b0ab17-2be1-431a-af35-6fe4d50cf924",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/08/fvytIn9aMXrhEWC.png\" alt=\"image-20211208193727983\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d73ad0-c926-40bd-8b82-0c61bd685311",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/08/rY72cSFqjgPpThf.jpg\" alt=\"111\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a381f1cf-bcfd-491b-a4db-4dcf93b602af",
   "metadata": {},
   "source": [
    "而在后续建模过程中，我们还将频繁使用这种手动集成的方式来提高模型效果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
